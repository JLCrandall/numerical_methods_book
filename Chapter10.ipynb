{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Systems of Nonlinear Equations\n",
      "\n",
      "To start our investigation into systems of nonlinear equations, let's consider the pair of nonlinear equations:\n",
      "\n",
      "$ x^2 + y^2 = 4$\n",
      "\n",
      "$e^x + y = 1$\n",
      "\n",
      "If we viewed these equations graphically we will see that these equations intersect near the points $(-1.8,0.8)$ and $(1,-1.7)$."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import Gnuplot\n",
      "\n",
      "x = np.linspace(0,0.5,3)\n",
      "y = np.sqrt(4-x^2)\n",
      "y1 = -np.sqrt(4-x^2)\n",
      "#creates Gnuplot object\n",
      "g = Gnuplot.Gnuplot(persist=1)\n",
      "\n",
      "#creates Gnuplot Data\n",
      "d1 = Gnuplot.Data(x, y1, with_='1p',title='d1')\n",
      "d2 = Gnuplot.Data(x, y2, with_= '1',title='d2')\n",
      "\n",
      "#Gnuplot formatting options\n",
      "g('set grid')\n",
      "g('set key left')\n",
      "\n",
      "#initiates the plot\n",
      "g.plot(d1,d2)\n",
      "\n",
      "#plot=plot[{sqrt{4-x^2}}, -sqrt{4-x^2}, 1-exp{x}, {x, -3, 3}]\n",
      "#plot=plot.range{{-3,3},{-3,3}}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "ImportError",
       "evalue": "No module named Gnuplot",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-6-6079e0255d9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mGnuplot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspae\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m^\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mImportError\u001b[0m: No module named Gnuplot"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We could take a Gauss-Siedel like approach by rearranging the equations as follows:\n",
      "\n",
      "$x = \\pm \\sqrt{4-y^2}$\n",
      "\n",
      "$y=1-e^x$\n",
      "\n",
      "We can use an initial guess $y_0$ to find $x_0$, then $x_0$ to find $y_1$ and so on."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Gauss-Siedel like Iteration Example\n",
      "\n",
      "If we do exactly as described above, we have the following using an initial guess of $y_0 = 0.8$. (We are shooting for the leftmost root)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### INTERACTIVE EXAMPLE ###"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It worked! Let's now try an initial guess of $y_0 = -1.7$ and shoot for the rightmost root."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### INTERACTIVE MODEL ###"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "What happened? Well, the method went unstable and ended up diverging into the imaginary plane. The actual numbers are shown below."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### INTERACTIVE VALUES ###"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "However, if we simply rewrite the equations as follows:\n",
      "\n",
      "$x = ln(1-y)$\n",
      "\n",
      "$y = \\pm \\sqrt{4-x^2}$\n",
      "\n",
      "We can use the initial guess of $y_0 = -1.7$ and it will converge to this solution."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### Interative Model ###"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This example illustrates some of the difficulties that can arise when trying to solve systems of nonlinear equations. Finding a convergent form of the equations gets increasingly difficult as the number of equations gets larger."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Newton's Method for Systems of Nonlinear Equations\n",
      "\n",
      "Consider the following systems of two nonlinear equations:\n",
      "\n",
      "$f(x,y) = 0$\n",
      "\n",
      "$g(x,y) = 0$\n",
      "\n",
      "Let $x = r, y = s$ and expand both functions as a Taylor series about the point $(x_i,y_i)$ in terms of $(r-x_i),(s-y_i)$, where $(x_i,y_i)$ is a point near the root. We have:\n",
      "\n",
      "$f(r,s) = f(x_i,y_i) + f_x(x_i,y_i)(r-x_i) + f_y(x_i,y_i)(s-y_i) + ... \\\\\n",
      "g(r,s) = g(x_i,y_i) + g_x(x_i,y_i)(r-x_i) + g_y(x_i,y_i)(s-y_i) + ...$\n",
      "\n",
      "If we keep only the lienar terms we can write the equations in matrix form as:\n",
      "\n",
      "$\\begin{pmatrix}0\\\\0\\end{pmatrix} = \\begin{pmatrix} f(x_i,y_i)\\\\g(x_i,y_i)\\end{pmatrix} + \\begin{pmatrix} f_x(x_i,y_i) & f_y(x_i,y_i)\\\\g_x(x_i,y_i) & g_y(x_i,y_i)\\end{pmatrix}\\begin{pmatrix}r-x_i\\\\s-y_i\\end{pmatrix}$\n",
      "\n",
      "or rearranging, we have:\n",
      "\n",
      "$-\\begin{pmatrix}f(x_i,y_i)\\\\g(x_i,y_i)\\end{pmatrix} = \\begin{pmatrix}f_x(x_i,y_i) & f_y(x_i,y_i)\\\\g_x(x_i,y_i) & g_y(x_i,y_i)\\end{pmatrix}\\begin{pmatrix}\\Delta x_i \\\\ \\Delta y_i \\end{pmatrix}$\n",
      "\n",
      "We can solve the equation above using Gaussian elimination for $(\\Delta x_i, \\Delta y_i)$ then the next approximation becomes:\n",
      "\n",
      "$x_{i+1} = x_{i} + \\Delta x_i \\\\ y_{i+1} = y_{i} + \\Delta y_i$\n",
      "\n",
      "This procedure can be generalized to $n$ number of equations in a straightforward manner."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Newton's Method Example\n",
      "\n",
      "Using Newton's method to solve the same example as before, we can see that it will converge to either root with an approximate guess."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### INTERACTIVE EXAMPLE ###"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### INTERACTIVE EXAMPLE ###"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Pseudocode for Newton's Method for Systems.\n",
      "\n",
      "To approximate the solution of the nonlinear system $F(\\bar{x}) = 0$. Given an initial guess vector $\\bar{x} = \\{x_1, x_2, ..., x_n\\}^T$\n",
      "\n",
      "  1. Set $k=1$\n",
      "  1. While $k < $ max iteration and $ERR > TOL$ then do Steps A-E\n",
      "    1. Calculate $F(\\bar{x})$ and $J(\\bar{x})$, where $J(\\bar{x})_{ij} = \\frac{\\delta}{\\delta x_j}f_{i}(\\bar{x})$\n",
      "    1. Solve the linearized equation $J(\\bar{x})\\Delta\\bar{x} = -F(\\bar{x})$\n",
      "    1. Set $\\bar{x} = x + \\Delta\\bar{x}$\n",
      "    1. $ERR = ||\\Delta\\bar{x}||$\n",
      "    1. $k = k+1$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Quasi-Newton Methods.\n",
      "\n",
      "A significant weakness of Newton's method for solving systems of nonlinear equations is that, for each iteration, a Jacobian matrix has to be computed and an $n$ x $n$ linear system solved. Let's consider the number of computations associated with one iteration of Newton's method. The Jacobian matrix associated with a system of $n$ nonlienar equations written in the form $F(x)=0$ requires that the $n^2$ partial derivatives of the $n$ component functions of $F$ to be determined and evaluated. When the exact evaluation of the partial derivatives is not easily accomplished we can use a finite difference approximation to the partial derivatives, for example:\n",
      "\n",
      "$\\frac{\\delta f_j}{\\delta x_k}(\\bar{x}) \\approx \\frac{f_j(\\bar{x}+\\bar{e}_k h)-f_j(\\bar{x})}{h}$\n",
      "\n",
      "where $h$ is small in absolute value and $\\bar{e}_k$ is the vector whose only nonzero entry is a 1 in the $k^{th}$ coordinate. This approximation still requires that at least $n^2$ scalar functional evaluations be performed to approximate the Jacobian matrix, this does not increase the efficiency of the computation.\n",
      "\n",
      "There are a class of methods known as *least-change secant updates* that produce algorithms called *quasi-Newton*. These methods replace the Jacobian matrix in Newton's method with an approximation matrix that is updated at each iteration. We lose the quadratic convergence of Newton's method with these techniques, but in general they are still superlinear. One of these quasi-Newton methods we will consider is called *Broyden's method*."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Broyden's Method\n",
      "\n",
      "Suppose we have an initial approximation $\\bar{x}^{(0)}$ to the solution of a system of nonlinear equations $F(x)=0$. We calculate the next approximations $\\bar{x}^{(1)}$ in exactly the same fasion as Newton's method, or in the case where the Jacobian matrix is difficult to explicity evaluate we can use the finite difference approximation described in the previous section. To compute $\\bar{x}^{(2)}$ we depart from Newton's method and examine the Secant method fora  singular nonlinear equation. The Secant method uses the approximation:\n",
      "\n",
      "$f'(x) \\approx \\frac{f(x_1)-f(x_0)}{x_1-x_0}$\n",
      "\n",
      "For nonlinear systems $\\bar{x}^{(1)}-\\bar{x}^{(0)}$ is a vector, and the corresponding quotient is undefined. However, the method proceeds similarly in that we replace the matrix $J(\\bar{x}^{(1)})$ in Newton's method for systems by a matrix $A_1$ with the property that:\n",
      "\n",
      "$A_1(\\bar{x}^{(1)}-\\bar{x}^{(0)}) = F(\\bar{x}^{(1)})-F(\\bar{x}^{(0)})$ ($*$)\n",
      "\n",
      "Any nonzero vector in $\\mathbb{R}^n$ can be written as the sum of a multiple of $\\bar{x}^{(1)}-\\bar{x}^{(0)}$ and a multiple of a vector in the orthogonal complement of $\\bar{x}^{(1)}-\\bar{x}^{(0)}$. So to uniquely define the matrix $A_1$, we need to specify how it acts on the orthogonal complement of $\\bar{x}^{(1)}-\\bar{x}^{(0)}$. There is no information available about the change in $F$ in a direction orthogonal to $\\bar{x}^{(1)}-\\bar{x}^{(0)}$, therefore we require that:\n",
      "\n",
      "$A_1 \\bar{z} = J(\\bar{x}^{(0)})\\bar{z}$, whenever $(\\bar{x}^{(1)}-\\bar{x}^{(0)})^T\\bar{z}=0$ ($**$)\n",
      "\n",
      "Thus, any vector orthogonal to $\\bar{x}^{(1)}-\\bar{x}^{(0)}$ is unaffected by the update from $J(\\bar{x}^{(0)})$, which was used to compute $\\bar{x}^{(1)}$, to $A_1$, which is used in the determination of $\\bar{x}^{(2)}$. Conditions ($*$) and ($**$) uniquely define $A_1$ as:\n",
      "\n",
      "$A_1 = J(\\bar{x}^{(0)}) + \\frac{[F(\\bar{x}^{(1)})-F(\\bar{x}^{(0)})-J(\\bar{x}^{(0)})(\\bar{x}^{(1)}-\\bar{x}^{(0)}](\\bar{x}^{(1)}-\\bar{x}^{(0)})^T}{||\\bar{x}^{(1)}-\\bar{x}^{(0)}||^2}$ ($***$)\n",
      "\n",
      "$A_1$ is then used in place of $J(\\bar{x}^{(1)})$ to determine $\\bar{x}^{(2)}$ as \n",
      "\n",
      "$\\bar{x}^{(2)} = \\bar{x}^{(1)} - A_1^{-1}F(\\bar{x}^{(1)})$\n",
      "\n",
      "For $\\bar{x}^{(3)}$ we replace $J(\\bar{x}^{(0)})$ with $A_1$. So in general we have:\n",
      "\n",
      "$A_i = A_{i-1} + \\frac{\\bar{y}_i-A_{i-1}\\bar{s}_i}{||\\bar{s}_i||^2}s_{i}^T$ and $\\bar{x}^{(i+1)} = \\bar{x}^{(i)}-A_i^{-1}F(\\bar{x}^{(i)})$\n",
      "\n",
      "where $\\bar{s}_i = \\bar{x}^{(i)} - \\bar{x}^{(i-1)}$ and $\\bar{y}_i = F(\\bar{x}^{(i)}) - F(\\bar{x}^{(i-1)})$.\n",
      "\n",
      "If the method is employed as outlined above the number of scalar functional evaluations is reduced from $n^2 + n$ to $n$, but there is still $O(n^3)$ computations required to solve the linear system:\n",
      "\n",
      "$A_i s_{i+1} = -F(\\bar{x}^{(i)})$\n",
      "\n",
      "Left alone, this would not justify its use because of the reduction from quadratic to superlinear convergence as opposed to just using Newton's method. We will see that we will greatly improve this method with the use of the Sherman-Morrison formula."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Sherman-Morrison Formula.\n",
      "\n",
      "The Sherman-Morrison formula states the following:\n",
      "\n",
      "If $A$ is a nonsingular matrix and $\\bar{x}$ and $\\bar{y}$ are vectors, the $A + \\bar{x}\\bar{y}^T$ is nonsingular, provided that $\\bar{y}^TA^{-1} \\neq -1$, and \n",
      "\n",
      "$ (A + \\bar{x}\\bar{y}^T)^{-1} = A^{-1} - \\frac{A^{-1}\\bar{x}\\bar{y}^TA^{-1}}{1+y^TA^{-1}\\bar{x}} s_i^T$\n",
      "\n",
      "This permits $A_i^{-1}$ to be computed directly from $A_{i-1}^{-1}$ bypassing the need for matrix inversion after the first iteration. Let's take a look at what happens when we apply the Sherman-Morrison formula to the general statement, solving for $A_i$ in Broyden's method.\n",
      "\n",
      "$A_i^{-1} = (A_{i-1} + \\frac{\\bar{y}_{i}-A_{i-1}\\bar{s}_i}{||\\bar{s}_i||^2} s_{i}^T)^{-1}$\n",
      "\n",
      "$ = A_{i-1}^{-1} - \\frac{}{}$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}