{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Systems of Nonlinear Equations\n",
      "\n",
      "To start our investigation into systems of nonlinear equations, let's consider the pair of nonlinear equations:\n",
      "\n",
      "$ x^2 + y^2 = 4$\n",
      "\n",
      "$e^x + y = 1$\n",
      "\n",
      "If we viewed these equations graphically we will see that these equations intersect near the points $(-1.8,0.8)$ and $(1,-1.7)$."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import Gnuplot\n",
      "\n",
      "x = np.linspace(0,0.5,3)\n",
      "y = np.sqrt(4-x^2)\n",
      "y1 = -np.sqrt(4-x^2)\n",
      "#creates Gnuplot object\n",
      "g = Gnuplot.Gnuplot(persist=1)\n",
      "\n",
      "#creates Gnuplot Data\n",
      "d1 = Gnuplot.Data(x, y1, with_='1p',title='d1')\n",
      "d2 = Gnuplot.Data(x, y2, with_= '1',title='d2')\n",
      "\n",
      "#Gnuplot formatting options\n",
      "g('set grid')\n",
      "g('set key left')\n",
      "\n",
      "#initiates the plot\n",
      "g.plot(d1,d2)\n",
      "\n",
      "#plot=plot[{sqrt{4-x^2}}, -sqrt{4-x^2}, 1-exp{x}, {x, -3, 3}]\n",
      "#plot=plot.range{{-3,3},{-3,3}}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "ImportError",
       "evalue": "No module named Gnuplot",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-1-5d6178849abb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mGnuplot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m^\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mImportError\u001b[0m: No module named Gnuplot"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We could take a Gauss-Siedel like approach by rearranging the equations as follows:\n",
      "\n",
      "$x = \\pm \\sqrt{4-y^2}$\n",
      "\n",
      "$y=1-e^x$\n",
      "\n",
      "We can use an initial guess $y_0$ to find $x_0$, then $x_0$ to find $y_1$ and so on."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Gauss-Siedel like Iteration Example\n",
      "\n",
      "If we do exactly as described above, we have the following using an initial guess of $y_0 = 0.8$. (We are shooting for the leftmost root)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### INTERACTIVE EXAMPLE ###"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It worked! Let's now try an initial guess of $y_0 = -1.7$ and shoot for the rightmost root."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### INTERACTIVE MODEL ###"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "What happened? Well, the method went unstable and ended up diverging into the imaginary plane. The actual numbers are shown below."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### INTERACTIVE VALUES ###"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "However, if we simply rewrite the equations as follows:\n",
      "\n",
      "$x = ln(1-y)$\n",
      "\n",
      "$y = \\pm \\sqrt{4-x^2}$\n",
      "\n",
      "We can use the initial guess of $y_0 = -1.7$ and it will converge to this solution."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### Interative Model ###"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This example illustrates some of the difficulties that can arise when trying to solve systems of nonlinear equations. Finding a convergent form of the equations gets increasingly difficult as the number of equations gets larger."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Newton's Method for Systems of Nonlinear Equations\n",
      "\n",
      "Consider the following systems of two nonlinear equations:\n",
      "\n",
      "$f(x,y) = 0$\n",
      "\n",
      "$g(x,y) = 0$\n",
      "\n",
      "Let $x = r, y = s$ and expand both functions as a Taylor series about the point $(x_i,y_i)$ in terms of $(r-x_i),(s-y_i)$, where $(x_i,y_i)$ is a point near the root. We have:\n",
      "\n",
      "$f(r,s) = f(x_i,y_i) + f_x(x_i,y_i)(r-x_i) + f_y(x_i,y_i)(s-y_i) + ... \\\\\n",
      "g(r,s) = g(x_i,y_i) + g_x(x_i,y_i)(r-x_i) + g_y(x_i,y_i)(s-y_i) + ...$\n",
      "\n",
      "If we keep only the lienar terms we can write the equations in matrix form as:\n",
      "\n",
      "$\\begin{pmatrix}0\\\\0\\end{pmatrix} = \\begin{pmatrix} f(x_i,y_i)\\\\g(x_i,y_i)\\end{pmatrix} + \\begin{pmatrix} f_x(x_i,y_i) & f_y(x_i,y_i)\\\\g_x(x_i,y_i) & g_y(x_i,y_i)\\end{pmatrix}\\begin{pmatrix}r-x_i\\\\s-y_i\\end{pmatrix}$\n",
      "\n",
      "or rearranging, we have:\n",
      "\n",
      "$-\\begin{pmatrix}f(x_i,y_i)\\\\g(x_i,y_i)\\end{pmatrix} = \\begin{pmatrix}f_x(x_i,y_i) & f_y(x_i,y_i)\\\\g_x(x_i,y_i) & g_y(x_i,y_i)\\end{pmatrix}\\begin{pmatrix}\\Delta x_i \\\\ \\Delta y_i \\end{pmatrix}$\n",
      "\n",
      "We can solve the equation above using Gaussian elimination for $(\\Delta x_i, \\Delta y_i)$ then the next approximation becomes:\n",
      "\n",
      "$x_{i+1} = x_{i} + \\Delta x_i \\\\ y_{i+1} = y_{i} + \\Delta y_i$\n",
      "\n",
      "This procedure can be generalized to $n$ number of equations in a straightforward manner."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Newton's Method Example\n",
      "\n",
      "Using Newton's method to solve the same example as before, we can see that it will converge to either root with an approximate guess."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### INTERACTIVE EXAMPLE ###"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### INTERACTIVE EXAMPLE ###"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Pseudocode for Newton's Method for Systems.\n",
      "\n",
      "To approximate the solution of the nonlinear system $F(\\bar{x}) = 0$. Given an initial guess vector $\\bar{x} = \\{x_1, x_2, ..., x_n\\}^T$\n",
      "\n",
      "  1. Set $k=1$\n",
      "  1. While $k < $ max iteration and $ERR > TOL$ then do Steps A-E\n",
      "    1. Calculate $F(\\bar{x})$ and $J(\\bar{x})$, where $J(\\bar{x})_{ij} = \\frac{\\delta}{\\delta x_j}f_{i}(\\bar{x})$\n",
      "    1. Solve the linearized equation $J(\\bar{x})\\Delta\\bar{x} = -F(\\bar{x})$\n",
      "    1. Set $\\bar{x} = x + \\Delta\\bar{x}$\n",
      "    1. $ERR = ||\\Delta\\bar{x}||$\n",
      "    1. $k = k+1$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Quasi-Newton Methods.\n",
      "\n",
      "A significant weakness of Newton's method for solving systems of nonlinear equations is that, for each iteration, a Jacobian matrix has to be computed and an $n$ x $n$ linear system solved. Let's consider the number of computations associated with one iteration of Newton's method. The Jacobian matrix associated with a system of $n$ nonlienar equations written in the form $F(x)=0$ requires that the $n^2$ partial derivatives of the $n$ component functions of $F$ to be determined and evaluated. When the exact evaluation of the partial derivatives is not easily accomplished we can use a finite difference approximation to the partial derivatives, for example:\n",
      "\n",
      "$\\frac{\\delta f_j}{\\delta x_k}(\\bar{x}) \\approx \\frac{f_j(\\bar{x}+\\bar{e}_k h)-f_j(\\bar{x})}{h}$\n",
      "\n",
      "where $h$ is small in absolute value and $\\bar{e}_k$ is the vector whose only nonzero entry is a 1 in the $k^{th}$ coordinate. This approximation still requires that at least $n^2$ scalar functional evaluations be performed to approximate the Jacobian matrix, this does not increase the efficiency of the computation.\n",
      "\n",
      "There are a class of methods known as *least-change secant updates* that produce algorithms called *quasi-Newton*. These methods replace the Jacobian matrix in Newton's method with an approximation matrix that is updated at each iteration. We lose the quadratic convergence of Newton's method with these techniques, but in general they are still superlinear. One of these quasi-Newton methods we will consider is called *Broyden's method*."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Broyden's Method\n",
      "\n",
      "Suppose we have an initial approximation $\\bar{x}^{(0)}$ to the solution of a system of nonlinear equations $F(x)=0$. We calculate the next approximations $\\bar{x}^{(1)}$ in exactly the same fasion as Newton's method, or in the case where the Jacobian matrix is difficult to explicity evaluate we can use the finite difference approximation described in the previous section. To compute $\\bar{x}^{(2)}$ we depart from Newton's method and examine the Secant method fora  singular nonlinear equation. The Secant method uses the approximation:\n",
      "\n",
      "$f'(x) \\approx \\frac{f(x_1)-f(x_0)}{x_1-x_0}$\n",
      "\n",
      "For nonlinear systems $\\bar{x}^{(1)}-\\bar{x}^{(0)}$ is a vector, and the corresponding quotient is undefined. However, the method proceeds similarly in that we replace the matrix $J(\\bar{x}^{(1)})$ in Newton's method for systems by a matrix $A_1$ with the property that:\n",
      "\n",
      "$A_1(\\bar{x}^{(1)}-\\bar{x}^{(0)}) = F(\\bar{x}^{(1)})-F(\\bar{x}^{(0)})$ ($*$)\n",
      "\n",
      "Any nonzero vector in $\\mathbb{R}^n$ can be written as the sum of a multiple of $\\bar{x}^{(1)}-\\bar{x}^{(0)}$ and a multiple of a vector in the orthogonal complement of $\\bar{x}^{(1)}-\\bar{x}^{(0)}$. So to uniquely define the matrix $A_1$, we need to specify how it acts on the orthogonal complement of $\\bar{x}^{(1)}-\\bar{x}^{(0)}$. There is no information available about the change in $F$ in a direction orthogonal to $\\bar{x}^{(1)}-\\bar{x}^{(0)}$, therefore we require that:\n",
      "\n",
      "$A_1 \\bar{z} = J(\\bar{x}^{(0)})\\bar{z}$, whenever $(\\bar{x}^{(1)}-\\bar{x}^{(0)})^T\\bar{z}=0$ ($**$)\n",
      "\n",
      "Thus, any vector orthogonal to $\\bar{x}^{(1)}-\\bar{x}^{(0)}$ is unaffected by the update from $J(\\bar{x}^{(0)})$, which was used to compute $\\bar{x}^{(1)}$, to $A_1$, which is used in the determination of $\\bar{x}^{(2)}$. Conditions ($*$) and ($**$) uniquely define $A_1$ as:\n",
      "\n",
      "$A_1 = J(\\bar{x}^{(0)}) + \\frac{[F(\\bar{x}^{(1)})-F(\\bar{x}^{(0)})-J(\\bar{x}^{(0)})(\\bar{x}^{(1)}-\\bar{x}^{(0)}](\\bar{x}^{(1)}-\\bar{x}^{(0)})^T}{||\\bar{x}^{(1)}-\\bar{x}^{(0)}||^2}$ ($***$)\n",
      "\n",
      "$A_1$ is then used in place of $J(\\bar{x}^{(1)})$ to determine $\\bar{x}^{(2)}$ as \n",
      "\n",
      "$\\bar{x}^{(2)} = \\bar{x}^{(1)} - A_1^{-1}F(\\bar{x}^{(1)})$\n",
      "\n",
      "For $\\bar{x}^{(3)}$ we replace $J(\\bar{x}^{(0)})$ with $A_1$. So in general we have:\n",
      "\n",
      "$A_i = A_{i-1} + \\frac{\\bar{y}_i-A_{i-1}\\bar{s}_i}{||\\bar{s}_i||^2}s_{i}^T$ and $\\bar{x}^{(i+1)} = \\bar{x}^{(i)}-A_i^{-1}F(\\bar{x}^{(i)})$\n",
      "\n",
      "where $\\bar{s}_i = \\bar{x}^{(i)} - \\bar{x}^{(i-1)}$ and $\\bar{y}_i = F(\\bar{x}^{(i)}) - F(\\bar{x}^{(i-1)})$.\n",
      "\n",
      "If the method is employed as outlined above the number of scalar functional evaluations is reduced from $n^2 + n$ to $n$, but there is still $O(n^3)$ computations required to solve the linear system:\n",
      "\n",
      "$A_i s_{i+1} = -F(\\bar{x}^{(i)})$\n",
      "\n",
      "Left alone, this would not justify its use because of the reduction from quadratic to superlinear convergence as opposed to just using Newton's method. We will see that we will greatly improve this method with the use of the Sherman-Morrison formula."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Sherman-Morrison Formula.\n",
      "\n",
      "The Sherman-Morrison formula states the following:\n",
      "\n",
      "If $A$ is a nonsingular matrix and $\\bar{x}$ and $\\bar{y}$ are vectors, the $A + \\bar{x}\\bar{y}^T$ is nonsingular, provided that $\\bar{y}^TA^{-1} \\neq -1$, and \n",
      "\n",
      "$ (A + \\bar{x}\\bar{y}^T)^{-1} = A^{-1} - \\frac{A^{-1}\\bar{x}\\bar{y}^TA^{-1}}{1+y^TA^{-1}\\bar{x}} s_i^T$\n",
      "\n",
      "This permits $A_i^{-1}$ to be computed directly from $A_{i-1}^{-1}$ bypassing the need for matrix inversion after the first iteration. Let's take a look at what happens when we apply the Sherman-Morrison formula to the general statement, solving for $A_i$ in Broyden's method.\n",
      "\n",
      "$A_i^{-1} = (A_{i-1} + \\frac{\\bar{y}_{i}-A_{i-1}\\bar{s}_i}{||\\bar{s}_i||^2} s_{i}^T)^{-1}$\n",
      "\n",
      "$ = A_{i-1}^{-1} - \\frac{A_{i-1}^{-1}\\left(\\frac{\\bar{y}_i-A_{i-1}\\bar{s}_i}{||\\bar{s}_i||^2}s_i^T\\right)A_{i-1}^{-1}}{1 + \\bar{s}_i^T A_{i-1}^{-1}\\left(\\frac{\\bar{y}_{i}-A_{i-1}\\bar{s}_i}{||\\bar{s}_i||^2}s_i^T\\right)}$\n",
      "\n",
      "$ = A_{i-1}^{-1} - \\frac{(A_{i-1}^{-1}\\bar{y}_i-\\bar{s}_i)s_i^T A_{i-1}^{-1}}{||\\bar{s}_i||^2 + \\bar{s}_i^T A_{i-1}^{-1} \\bar{y}_i - ||\\bar{s}_i||^2}$\n",
      "\n",
      "$ = A_{i-1}^{-1} + \\frac{(\\bar{s}_i-A_{i-1}^{-1}\\bar{y}_i)\\bar{s}_i^T A_{i-1}^{-1}}{\\bar{s}_i^T A_{i-1}^{-1}\\bar{y}_i}$\n",
      "\n",
      "This computation involves only matrix-vector multiplication at each step and therefore requires only $O(n^2)$ an arithmetic calculations. We can bypass the calculation of $A_i$ and the necessity of solving the linear system, greatly speeding up the Broyden's method."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Pseudocode for Broyden's method.\n",
      "\n",
      "To approximate the solution of the nonlinear system $F(x) = 0$, given an initial guess $\\bar{x}=(x_1, x_2, ..., x_n)^T$\n",
      "\n",
      "  1. Set $A_0 = J(x)$, where $J(\\bar{x})_{ij} = \\frac{\\delta}{\\delta x_j} f_i(\\bar{x})$\n",
      "  1. Set $\\bar{v} = F(\\bar{x})$\n",
      "  1. Set $A = A_0^{-1}$ (must solve for $A_0^{-1}$)\n",
      "  1. Set $\\bar{s} = -A\\bar{v}$\n",
      "  1. Set $\\bar{x} = \\bar{x} + \\bar{s}$\n",
      "  1. Set $k = 2$\n",
      "  1. While $k < $ max iterations do Steps A-K\n",
      "    1. Set $\\bar{w} = \\bar{v}$\n",
      "    1. Set $\\bar{v} = F(\\bar{x})$\n",
      "    1. Set $\\bar{y} = \\bar{v} - \\bar{w}$\n",
      "    1. Set $\\bar{z} = -A\\bar{y}$\n",
      "    1. Set $p = -\\bar{s}^T \\bar{z}$\n",
      "    1. Set $\\bar{u}^T = \\bar{s}^T A$\n",
      "    1. Set $A = A + \\frac{1}{p}(\\bar{s} + \\bar{z})\\bar{u}^T$\n",
      "    1. Set $\\bar{s} = - A\\bar{v}$\n",
      "    1. Set $\\bar{x} = \\bar{x} + \\bar{s}$\n",
      "    1. If $||\\bar{s} || < TOL$, stop program and output $\\bar{x}$\n",
      "    1. Set $k = k+1$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Steepest Descent Techniques\n",
      "\n",
      "We have already discussed the advantages of the fast convergence rates of Newton's method and quasi-Newton's methods, but a weakness of these methods for solving systems of nonlinear equations is the same problem that arises in single variable non-linear equations in that a close approximation to the solution is necessary for these methods to converge. The *Steepest Descent Technique* presented here only converges linearly to the solution, but will usually converge even with a poor apprximation. Therefore it is often used much like the Bisection method for single variables, in order to get a good approximation to a root, before handing the algorithm off to one of the other methods in order to \"polish off\" the solution. The Steepest Descent Technique is an optimation technique that determines the local minimum for a multivariable function of the form $g :\\mathbb R^n \\rightarrow \\mathbb R$. This method has been used outside of solving nonlinear systems of equations. Consider the system of equations below:\n",
      "\n",
      "$\\begin{matrix}f_1(x_1, x_2, ..., x_n) = & 0\\\\\n",
      "f_2(x_1, x_2, ..., x_n) = & 0\\\\\n",
      "\\vdots & \\vdots\\\\\n",
      "f_n(x_1, x_2, ..., x_n) = & 0\\end{matrix}$\n",
      "\n",
      "These equations have a solution $\\bar{x} = (x_1, x_2, ..., x_n)^T$ exactly when the function $g$ defined by \n",
      "\n",
      "$g(x_1, x_2, ..., x_n) = \\sum_{i=1}^n [f_{j}(x_1, x_2, ..., x_n)]^2$\n",
      "\n",
      "has a minimum value 0. We know from calculus and the Extreme Value Theorem, that a differentiable single-variable function can have a local minimum only when the derivative of that function is zero. We can extend this to multivariable functions by considering the gradient: \n",
      "\n",
      "$\\triangledown g(x) = \\left(\\frac{\\partial g(x)}{\\partial x_1}, \\frac{\\partial g(x)}{\\partial x_2}, ..., \\frac{\\partial g(x)}{\\partial x_n} \\right)^T$\n",
      "\n",
      "A differentiable multivariable function can have a local minimum at $\\bar{x}$ only when $\\triangledown g(\\bar{x}) = 0$. The gradient has another important property associated with the minimization of multivariable functions. Let $\\bar{v} = (v_1, v_2, ..., \\bar{v}_n)^T$ be a unit in $\\mathbb R^n$; that is:\n",
      "\n",
      "$||\\bar{v}||^2 = \\sum_{i=1}^n v_{i}^2 = 1$\n",
      "\n",
      "The *directional derivative* of $g$ at $\\bar{x}$ in the direction of $\\bar{v}$ is defined by \n",
      "\n",
      "$D_{\\bar{v}} g(\\bar{x}) = lim_{h\\rightarrow 0} \\frac{1}{h} [g(\\bar{x} + h\\bar{v}) - g(\\bar{x})] = \\bar{v}^T \\cdot \\triangledown g(\\bar{x})$\n",
      "\n",
      "The directional derivative of $g$ at $\\bar{x}$ in the direction of $\\bar{v}$ measures the change in the value of the function at $g$ relative to the change in the direction of $\\bar{v}$. It turns out that the direction taht produces the maximum value for the directional derivative (the fastest change in $g$) occurs when $\\bar{v}$ is chosen to be parallel with $\\triangledown g(\\bar{x})$. So if we are going to devise an iterative technique that minimized $g$ we want to move at each step in the direction of $-\\triangledown g(\\bar{x})$ in order to get to 0 the fastest. Therefore an appropriate choice for $\\bar{x}^{(1)}$ given an initial guess $\\bar{x}^{(0)}$ is:\n",
      "\n",
      "$\\bar{x}^{(1)} = \\bar{x}^{(0)} - \\alpha \\triangledown g(\\bar{x}^{(0)})$ \n",
      "\n",
      "where $\\alpha$ represents the magnitude of distance we want to move in one step. We want to chose an $\\alpha$ that produces a $g(\\bar{x}^{(1)})$ that is as far as possible from $g(\\bar{x}^{(0)})$. Another way of stating this is saying that we wish to now minimize the single variable equation:\n",
      "\n",
      "$h(\\alpha) = g(\\bar{x}^{(0)} - \\alpha \\triangledown g(\\bar{x}^{(0)}))$\n",
      "\n",
      "Finding a minimal value for $h$ directly would require differentiating $h$ and then solving a root-finding problem to determine the critical points of $h$. This procedure is too computationally expensive, instead, we choose three numbers $\\alpha_1 < \\alpha_2 < \\alpha_3$ taht, we hope, are close to the minimum of $h(\\alpha)$. Then we construct the quadratic polynomial $P(x)$ that interpolates $h$ at $\\alpha_1, \\alpha_2, \\alpha_3$. We define $\\hat{\\alpha}$ in $[\\alpha_1, \\alpha_3]$ so that $P(\\hat{\\alpha})$ is a minimum in the interval and use $P(\\hat{\\alpha})$ to approximate the minimum (this will be close enough to the maximum distance between $g(\\bar{x}^{(1)})$ and $g(\\bar{x}^{(0)})$. Then we use $\\hat{\\alpha}$ to determine the new iterate.\n",
      "\n",
      "$\\bar{x}^{(1)} = \\bar{x}^{(0)} - \\hat{\\alpha} \\triangledown g(\\bar{x}^{(0)})$\n",
      "\n",
      "Since $g(\\bar{x}^{(0)})$ can be evaluated, we can first choose $\\alpha_1 = 0$ to minimize $h$. Next a number $\\alpha_3$ is found with $h(\\alpha_3) < h(\\alpha_1)$. Finally, $\\alpha_2$ is chosen to be $\\frac{\\alpha_3}{2}$. The minimum value of $P$ on the interval will always occur at the only critical point of $P$ or at $\\alpha_3$ because, by assumption, $P(\\alpha_3) = h(\\alpha_3) < h(\\alpha_1) = P(\\alpha_1)$. The critical point of $P$ is easy to find since $P$ is a quadratic polynomial."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}