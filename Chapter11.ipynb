{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Numerical Differentiation\n",
      "\n",
      "The derivative of the function $f$ at $x_0$ is\n",
      "\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$f'(x_0) = lim_{h\\rightarrow 0} \\frac{f(x_{0}+h)-f(x_0)}{h}$.\n",
      "\n",
      "The formula gives us an easy way to generate an approximate to $f'(x)$; compute\n",
      "\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\frac{f(x_0 +h)-f(x_0)}{h}$\n",
      "\n",
      "for small values of $h$. This method seems obvious, but unfortunately it is not very accurate due to roundoff error.\n",
      "\n",
      "If a function is reasonably well approximated by an interpolating polynomial, we should expect the slope of the function to also be approximated by the slope of the polynomial. So we'll start with considering a divided-difference polynomial for approximating a polynomial."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Divided-difference polynomials\n",
      "\n",
      "Assume that a function $f(x)$ is known at several distinct values of $x$.\n",
      "\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\begin{matrix} x_0 & f_0\\\\x_1 & f_1\\\\x_2 & f_2\\\\x_3 & f_3\\end{matrix}$\n",
      "\n",
      "The $x$'s need not be equispaced nor even arranged in a monotonic order. Consider the $n^{th}$ degree polynomial:\n",
      "\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$P_n(x) = a_0 + (x-x_0)a_1 + (x-x_0)(x-x_1)a_2 + ... + (x-x_0)(x-x_1)...(x-x_{n-1})a_{n}$. \n",
      "\n",
      "If we chose the $a_i$ so that $P_n(x)$ equals $f(x)$ at the $n+1$ known points, $x_0, x_1, ..., x_n$, the $P_n(x)$ is an interpolating polynomial. The $a_i$ can be readily determined by using what are called the divided differences of the tabulated values. We will use the notation:\n",
      "\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$f[x_0,x_1] = \\frac{f_1-f_0}{x_1-x_0}$\n",
      "\n",
      "This is called the first divided difference between $x_0$ and $x_1$.\n",
      "\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$f[x_1,x_2] = \\frac{f_2-f_1}{x_2-x_1}$\n",
      "\n",
      "is called the first divided difference between $x_1$ and $x_2$. In general, \n",
      "\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$f[x_s,x_t] = \\frac{f_t-f_s}{x_t-x_s}$\n",
      "\n",
      "is the first divided difference between $x_s$ and $x_t$. Second- and higher-order differences are defined in terms of lower-order differences. For example:\n",
      "\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$f[x_0,x_1,x_2] = \\frac{f[x_1,x_2]-f[x_0,x_1]}{x_2-x_0}$,\n",
      "\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$f[x_0,x_1,...,x_i] = \\frac{f[x_1,x_2,...,x_i]-f[x_0,x_1,...,x_{i-1}]}{x_{i}-x_0}$\n",
      "\n",
      "We can construct a table with these divided differences, an example:\n",
      "\n",
      "$$\n",
      "\\begin{matrix} x_i & f_i & f[x_i, x_{i+1}] & f[x_i,x_{i+1},x_{i+2}] & f[x_i, x_{i+1}, x_{i+2},x_{i+3}] & f[x_i, x_{i+1}, x_{i+2}, x_{i+3}, x_{i+4}]\\\\\n",
      "3.2 & 22.0\\\\\n",
      "2.7 & 17.8 & 8.4\\\\\n",
      "1.0 & 14.2 & 2.118 & 2.856\\\\\n",
      "4.8 & 38.3 & 6.342 & 2.012 & -0.528\\\\\n",
      "5.6 & 51.7 & 16.750 & 2.263 & 0.0865 & 0.256\\end{matrix}\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Divided-difference polynomials\n",
      "\n",
      "We can now find the $a_i$'s. We have,\n",
      "\n",
      "$$\n",
      "\\begin{matrix} x=x_0 & : & P_n (x_0) = a_0\\\\\n",
      "x = x_1 & : & P_n(x_1) = a_0 + (x_1 - x_0)a_1\\\\\n",
      "x = x_2 & : & P_n(x_2) = a_0 + (x_1 - x_0) + (x_2 - x_0)(x_2 - x_1)a_2\\\\\n",
      " & \\vdots & \\\\\n",
      "x = x+3 & : & P_n(x_n) = a_0 + (x_n - x_0)a_1 + (x_n-x_0)(x_n-x_1)a_2 + ... + (x_n-x_0)...(x_n-x_{n-1})a_{n}\\end{matrix}\n",
      "$$\n",
      "\n",
      "If each $P_n(x)$ is to be an interpolating polynomail, it must match the table for all $n+1$ entries:\n",
      "\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$P_n(x_i)=f_i$ for $i = 1, 2, ..., n$.\n",
      "\n",
      "If $P_n(x_i)$ in the equation is replaced by $f_i$, we get a triangular system, and each $a_i$ can be computed in turn."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Newton-Gregory Interpolating Polynomial\n",
      "\n",
      "Many times the data we wish to interpolate is sampled at equal intervals. If this is true, the problem of interpolation is considerably simplified. Here we require that the data is ordered in a monotonically increasing fashion with $x$. If we let the uniform difference between the $x$ values equal $h = \\Delta x$, then using subscripts to represent the order of the $x$ and $f(x)$ values, we define the first differences of the function as \n",
      "\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\Delta f_0 = f_1 - f_0$, $\\Delta f_1 = f_2 - f_1$, $...$, $\\Delta f_i = f_{i+1} - f_{i}$\n",
      "\n",
      "The second- and higher-order differences are defined similarly:\n",
      "\n",
      "$$\n",
      "\\Delta^2 f_1 = \\Delta(\\Delta f_1) = \\Delta(f_2 - f_1) = \\Delta f_2 - \\Delta f_1 = (f_3 - f_2) - (f_2 - f_1) = f_3 - 2f_2 - f_1\\\\\n",
      "\\Delta^2 f_i = f_{i+2} - 2f_{i+1} - f_i\\\\\n",
      "\\Delta^3 f_i = f_{i+3} - 3f_{i+2} + 3f_{i+1} - f_{i}\\\\\n",
      "\\vdots \\\\\n",
      "\\Delta^n f_i = f_{i+n} - nf_{i+n-1} + \\frac{n(n-1)}{2!} f_{i+n-2} - \\frac{n(n-1)(n-2)}{3!} f_{i+n-3} + ...\n",
      "$$\n",
      "\n",
      "Using this information the divided-difference interpolating polynomial can be written as follows, commonly called the Newton-Gregory forward polynomial:\n",
      "\n",
      "$$\n",
      "\\begin{matrix} P_n(x_s) & = & f_0 + s \\Delta f_0 + \\frac{s(s-1)}{2!} \\Delta^2 f_0 + \\frac{s(s-1)(s-2)}{3!} \\Delta^3f_0 + ...\\\\\n",
      " & = & f_0 + \\begin{pmatrix} s\\\\1\\end{pmatrix}\\Delta f_0 + \\begin{pmatrix}s\\\\2\\end{pmatrix}\\Delta^2f_0 + \\begin{pmatrix}s\\\\3\\end{pmatrix}\\Delta^3f_0 + \\begin{pmatrix}s\\\\4\\end{pmatrix}\\Delta^4f_0 + ...\\end{matrix}\n",
      " $$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Error of polynomial interpolation\n",
      "\n",
      "We will develop an expression for the error of $P_n(x)$. We write the error function in a form that has the property that is equivalent to zero at the $n+1$ points, from $x_0$ through $x_n$, where $P_n(x)$ and $f(x)$ are the same. We call this function $E(x)$:\n",
      "\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$E(x) = f(x) - P_n(x) = (x-x_0)(x-x_1)...(x-x_n)g(x)$\n",
      "\n",
      "the $n+1$ linear factors give $E(x)$ the zeros we know it must have and $g(x)$ accounts for its behavior at values other than at $x_0, x_1, ..., x_n$. Obviously, $f(x)-P_n(x)-E(x) = 0$, so\n",
      "\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$f(x) - P_n(x)-(x-x_0)(x-x_1)...(x-x_n)g(x)=0$(*)\n",
      "\n",
      "In order to thetermine $g(x)$ we will construct an auxiliary function $W(x,t)$\n",
      "\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$W(t) = f(t) - P_n(t)-(t-x_0)(t-x_1)...(t-x_n)g(x)$.\n",
      "\n",
      "At $t=x_0, x_1,...,x_n$, the $W$ function is zero $(n+1$ times), but it also has a zero if $t=x$ by definition from (*). There are a total of $n+2$ values of $t$ that make $W(t) = 0$ hold. We now require that $W(t)$ is continuous and differentiable. This implies that there is a zero to its derivative $W'(t)$ between each of the $n+2$ zeros of $W(t)$, a total of $n+1$ zeros. If $W''(t)$ exists, and we require it does, there will be $n$ zeros of it and likewise $n-1$ zeros of $W'''(t)$ and so on, until we reach $W^{(n+1)}(t)$, which must have at least one zero in the interval that has $x_0, x_n,$ or $x$ as endpoints. Call this value $t = \\xi$. We have\n",
      "\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$W^{(n+1)}(\\xi) = 0 = \\frac{d^{n+1}}{dt^{n+1}}[f(t)-P_n(t)-(t-x_0)...(t-x)g(x)]_{t=\\xi}$\n",
      "\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$ = f^{(n+1)}(\\xi)-0-(n+1)!g(x)$\n",
      "\n",
      "We now have:\n",
      "\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$g(x) = \\frac{f^{(n+1)}(\\xi)}{(n+1)!}, \\xi$ between $(x_0, x_n, x)$\n",
      "\n",
      "The conditions on $W(t)$ that are required for this development will be met if $f(x)$ has these same properties, because $P_n(x)$ is continuous and differentiable. We now have our error term:\n",
      "\n",
      "$E(x) = (x-x_0)(x-x_1)...(x-x_n)\\frac{f^{(n+1)}(\\xi)}{(n+1)!}$ with $\\xi$ on the smallest interval that contains $\\{x,x_0,x_1,...,x_n\\}$.\n",
      "\n",
      "This error term is not always extremely useful because the actual function that generates the $x_i, f_i$ values is often unknown, therefore we cannot know its $n+1$ derivative. We can conclude, however, that if the function is smooth, a low-degree polynomial should work satisfactorily. On the other hand, a non-smooth function can be expected to have larger errors when interpolated."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Returning to Numerical Derivatives\n",
      "\n",
      "For the polynomial,\n",
      "\n",
      "$f(x) = P_n(x) + E(x)$\n",
      "\n",
      "Differentiating gives:\n",
      "\n",
      "$P'_n(x) = f[x_0,x_1] + f[x_0,x_1,x_2](2x-x_0-x_1)+..._f[x_0,x_1,...,x_n]\\sum_{k=0}^{n-1} \\frac{(x-x_{0})...(x-x_{n-1})}{(x-x_{k})}$\n",
      "\n",
      "Error of $P'_n(x) = \\frac{f^{(n+1)}(\\xi)}{(n+1)!} \\sum_{k=0}^n \\frac{(x-x_0)...(x-x_n)}{(x-x_k)}+\\frac{(x-x_0)...(x-x_n)}{(n+1)!}\\frac{d}{dx}[f^{(n+1)}(\\xi)]$\n",
      "\n",
      "Then,\n",
      "\n",
      "$f'(x) = P'_n(x)+E'(x)$\n",
      "\n",
      "if we evaluate at $x=x_0$ we can simplify,\n",
      "\n",
      "$f'(x_0) = f[x_0,x_1]+f[x_0,x_1,x_2](x-x_0)+...+$\n",
      "\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$f[x_0,x_1,...,x_n](x_0-x_1)(x_0-x_2)...(x_0-x_{n-1})+(x_0-x_1)...(x_0-x_n)\\frac{f^{(n+1)}(\\xi)}{(n+1)!}$\n",
      "\n",
      "From here forward we will assume equispaced points and make use of hte Newton-Gregory forward polynomial. Starting with the Newton-Gregory formula:\n",
      "\n",
      "$f(x_s) = P_n(x_s) + error = f_0 + \\begin{pmatrix}s\\\\1\\end{pmatrix}\\Delta f_0 + \\begin{pmatrix}s\\\\2\\end{pmatrix}\\Delta^2f_0 + ... + \\begin{pmatrix}s\\\\n\\end{pmatrix}\\Delta^nf_0 + error$\n",
      "\n",
      "The error of $P_n(x_s)$ is then\n",
      "\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Error of $P_n(x_s) = \\begin{pmatrix}s\\\\n+1\\end{pmatrix}h^{n+1}f^{(n+1)}(\\xi)$, $x_0 < \\xi < x_n$.\n",
      "\n",
      "The derivatives are then,\n",
      "\n",
      "$f'(x_s) = P'_n(x_{s}) = \\frac{d}{dx}(P_n(x_s)) = \\frac{d}{ds}(P_n(x_s))\\frac{ds}{dx} = \\frac{d}{ds}(P_n(x_s))$\n",
      "\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$ = \\frac{1}{h}(\\Delta f_0 + \\frac{1}{2}(s-1+s)\\Delta^2 f_0 + \\frac{1}{6}((s-1)(s-2)+s(s-2)+s(s-1)\\Delta^3f_0+...)$\n",
      "\n",
      "Or if we let $s = 0$, giving the derivative corresponding to $x_0$, we have:\n",
      "\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$f'(x_0) = \\frac{1}{h}(\\Delta f_0 - \\frac{1}{2}\\Delta^2 f_0 + \\frac{1}{3}\\Delta^3 f_0 + ... \\pm \\Delta^nf_0)$\n",
      "\n",
      "and the error:\n",
      "\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Error of $P'_n(x_{s}) = h^{n+1}f^{n+1}(\\xi)\\left[\\frac{d}{ds}\\begin{pmatrix}s\\\\n+1\\end{pmatrix}\\right]\\frac{1}{h} + \\begin{pmatrix}s\\\\n+1\\end{pmatrix}h^{n+1} \\frac{d}{dx}[f^{(n+1)}(\\xi)]$\n",
      "\n",
      "The second term in the error formula cannot be evaluated because the way $\\xi$ varies with $x$ is unknown, but when we let $s=0$, this second term vanishes because\n",
      "\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\begin{pmatrix}s\\\\n+1\\end{pmatrix} = \\frac{1}{(n+1)!}s(s-1)...(s-n) = 0$\n",
      "\n",
      "at s = 0.\n",
      "\n",
      "Finally, we have.\n",
      "\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Error of $P'_n(x_0) = h^{n+1}f^{(n+1)}(\\xi)\\left[(-1)^n \\frac{n!}{(n+1)!}\\right] (\\frac{1}{h}) = \\frac{(-1)^n}{n+1} h^n f^{(n+1)}(\\xi)$.\n",
      "\n",
      "Note that even though the interpolating polynomial gives the function exactly at $s=0$, our derivative formula is subject to an error $O(h^n)$ at that point, unless $f^{(n+1)}(\\xi) = 0$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Symbolic Operators\n",
      "\n",
      "When we introduce the Newton-Gregory interpolating polynomial earlier we used the forward differencing operator $\\Delta$, we can use the backward-differencing operator $\\triangledown$ and a *stepping operator* $E$. These are defined by their actions on the functions below:\n",
      "\n",
      "\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $\\Delta f(x_0) = f(x_0 + h) - f(x_0)$\n",
      "\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $\\Delta^2f(x_0) = \\Delta[\\Delta f(x_0)] = \\Delta f(x_0+h) - \\Delta f(x_0)$\n",
      "\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $\\triangledown f(x_0) = f(x_0) - f(x_0-h)$\n",
      "\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $\\triangledown^2f(x_0)=\\triangledown[\\triangledown f(x_0)] = \\triangledown f(x_0-h)$\n",
      "\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $E f(x_0) = f(x_0 + h)$\n",
      "\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $E^2f(x_0)= E[Ef(x_0)]=Ef(x_0+h)=f(x_0+2h)$\n",
      "\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $E^nf(x_0)= f(x_0 + nh)$\n",
      "\n",
      "\n",
      "There are also relationships between the operators:\n",
      "\n",
      "\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $\\Delta f(x_0) = Ef(x_0) - f(x_0) = (E-1)f(x_0)$\n",
      "\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $\\triangledown f(x_0) = f(x_0) - E^{-1}f(x_0) = (1-E^{-1}) f(x_0)$\n",
      "\n",
      "\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $\\Delta = E-1$\n",
      "\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $\\triangledown = 1-E^{-1}$\n",
      "\n",
      "\n",
      "We will find these relationships useful shortly."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Formulas for higher derivatives\n",
      "\n",
      "We can get formulas for higher derivatives by further differentiating the relationships we have already developed.\n",
      "\n",
      "\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $f''(x_s) = P''_n (x_s) = \\frac{1}{h^2} \\frac{d^2}{ds^2} P_n (x_s)$\n",
      "\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $= \\frac{1}{h^2}\\left(\\frac{1}{2}(2)\\Delta^2f_0 + \\frac{1}{6}[(s-2)+(s-1)+(s-2)+s+(s-1)+s]\\Delta^3f_0+...\\right)$\n",
      "\n",
      "At $s=0$:\n",
      "\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $f''(x_0) = \\frac{1}{h^2}(\\Delta^2f_0-\\Delta^3f_0+...$\n",
      "\n",
      "There is no simple pattern for the coefficients, and the error terms have very complicated coefficients. The situation gets even more complex as we move to higher and higher derivatives, so we would prefer a simpler method. Here is where we can utilize the symbolic operators:\n",
      "\n",
      "\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $E = 1+\\Delta$\n",
      "\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $y_s = E^s y_0$\n",
      "\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $y'_s = \\frac{d}{dx}(E^s y_0) = \\frac{1}{h} \\frac{d}{ds} (E^s y_0) = \\frac{1}{h}(ln E)E^sy_0$\n",
      "\n",
      "\n",
      "At $s=0$:\n",
      "\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $y'_0 = \\frac{1}{h}(ln E)y_0 = \\frac{1}{h} ln(1+\\Delta)y_0$\n",
      "\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $=\\frac{1}{h} (\\Delta y_0 - \\frac{1}{2}\\Delta^2y_0 + \\frac{1}{3} \\Delta^3 y_0 - \\frac{1}{4}y_0 + ...)$\n",
      "\n",
      "\n",
      "Where we have used a Maclaurin expansion for $ln(1+\\Delta)$. This result is exactly what we derived previously by nonsymbolic means. Let us now introduce a derivative symbolic operator $D$.\n",
      "\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $D y_0 = \\frac{1}{h} ln(1+\\Delta) y_0$\n",
      "\n",
      "We can abstract the equivalence:\n",
      "\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $D = \\frac{1}{h}ln(1+\\Delta)$\n",
      "\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $D^2=\\frac{1}{h}ln^2(1+\\Delta)$\n",
      "\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $D^3=\\frac{1}{h}ln^3(1+\\Delta)$\n",
      "\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\vdots$\n",
      "\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $D^n=\\frac{1}{h}ln^n(1+\\Delta)$\n",
      "\n",
      "Now we have the tools to more easily derive higher derivatives:\n",
      "\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $D^2 y_0 = \\frac{1}{h^2}(\\Delta - \\frac{1}{2}\\Delta^2 + \\frac{1}{3}\\Delta^3 - \\frac{1}{4}\\Delta^4+...)^2y_0$\n",
      "\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $=\\frac{1}{h^2} (\\Delta^2 - \\Delta^3 + \\frac{11}{12}\\Delta^4 - \\frac{5}{6}\\Delta^5 + ...)y_0$\n",
      "\n",
      "The first term will always have $\\Delta^n$, therefore a first apprximation to the $n^{th}$ derivative is then.\n",
      "\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$D^n y = \\frac{1}{h^n} \\Delta^n y + error O(h)$"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}