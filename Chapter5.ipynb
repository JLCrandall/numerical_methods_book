{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##The Eigenvalue Problem\n",
      "\n",
      "The eigenvalue problem was discussed previously in conjunction with the convergence of iterative methods in the solution of linear systems. As engineers we are often introduced to the eigenproblem in mechanics courses as the principal values and directions of the moment of inertia tensor, the stress and strain tensors, and as natural frequencies and modes in vibration theory. From a mathematical standpoint the eigenproblem can provide an elegant method viewing the structure of a matrix. A more formal introduction to the problem and the moethods one might use to solve the eigenproblem is the subject of the next Chapter.\n",
      "\n",
      "First, a short refresher. Suppose a scalar, $\\lambda$, and a vector $\\bar{x}$ are found to satisfy the matrix equation:\n",
      "\n",
      "$A\\bar{x}=\\lambda\\bar{x}$\n",
      "\n",
      "Then $\\lambda$ is said to be an *eigenvalue* and $\\bar{x}$ an *eigenvector* of $A$. We can rearrange this equation to be in the equivalent form:\n",
      "\n",
      "$(A-\\lambda I)\\bar{x} = 0$\n",
      "\n",
      "In order for a non-trivial solution to exist then,\n",
      "\n",
      "$\\det(A-\\lambda I) = 0$\n",
      "\n",
      "which results in a polynomial equation in $\\lambda$ known as the *characteristic polynomial*. The roots of the characteristic polynomial can be found to solve for the eigenvalues of $A$. This would be a very direct method for finding the eigenvalues of $A$ and typically how we may have been taught to do it in vibrations class or when solving for principal stresses in mechanics and materials. However, for large $A$, the problem becomes intractable to proceed by hand and we must resort to the computer for help. The problem is, that finding determinants of matrices is somewhat computationally expensive and finding good approximations to the roots of the characteristic polynomail can be difficult (we shall see when we cover nonlinear root solving). There are other computational ways of estimating the eigenvalues of a matrix, these methods are discussed in the following slides."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##The Power Method\n",
      "\n",
      "The Power Method is an iterative technique used to determine the eigenvalue of largest magnitude in a matrix. Once this method is understood, we will modify it slightly to determine other eigenvalues. This method also produces an associate eigenvector. Sometimes this method is used to find an eigenvector when an eigenvalue is known or found by other means. Let us assume that an $nxn$ matrix $A$ has $n$ eigenvalues, $\\lambda_1, \\lambda_2, ..., \\lambda_n$ with an associated collection of linearly independent eigenvectors $\\bar{v}_1, \\bar{v}_2, ...,\\bar{v}_n$. Let us also make the assumption that $\\lambda_1$ is the largest in magnitude, meaning $|\\lambda_1| > |\\lambda_2| \\geq ... \\geq |\\lambda_n|$. If we define a vector $\\bar{x}$ in $\\mathbb{R}^n$, becausethe eigenvectors are linear independent, it implies that constants $\\beta_1, \\beta_2, ..., \\beta_n$ exist such that,\n",
      "\n",
      "$\\bar{x} = \\sum_{j=1}^n\\beta_j\\bar{v}_j$\n",
      "\n",
      "Multiplying this equation on both sides by $A, A^2, ..., A^k$ we have:\n",
      "\n",
      "$$\n",
      "A\\bar{x} = \\sum_{j=1}^n\\beta_jA\\bar{v}_j = \\sum_{j=1}^n\\beta_j\\lambda_j\\bar{v}_j\\\\\n",
      "A^2\\bar{x} = \\sum_{j=1}^n\\beta_jA\\bar{v}_j = \\sum_{j=1}^n\\beta_j\\lambda_j^2\\bar{v}_j\\\\\n",
      "\\vdots\\\\\n",
      "A^k\\bar{x} = \\sum_{j=1}^n\\beta_jA\\bar{v}_j = \\sum_{j=1}^n\\beta_j\\lambda_j^k\\bar{v}_j\\\\\n",
      "$$\n",
      "\n",
      "Now let us factor out $\\lambda_1^k$ from the right hand side of the last equation.\n",
      "\n",
      "$A^k\\bar{x} = \\lambda_1^k\\sum_{j=1}^n\\beta_j(\\frac{\\lambda_j}{\\lambda_1})^k\\bar{v}_j$\n",
      "\n",
      "Now take the limit of both sides as $k \\rightarrow \\infty$.\n",
      "\n",
      "$\\lim_{k\\rightarrow\\infty}A^k\\bar{x} = lim_{k\\rightarrow\\infty}\\lambda_1^k\\sum_{j=1}^n\\beta_j(\\frac{\\lambda_j}{\\lambda_1})^k\\bar{v}_j$\n",
      "\n",
      "For $j = 2, 3, ..., n$ we have $lim_{k\\rightarrow\\infty}(\\frac{\\lambda_j}{\\lambda_1})^k = 0$ because $|\\lambda_1| > |\\lambda_j|$, that leaves\n",
      "\n",
      "$lim_{k\\rightarrow\\infty}A^k\\bar{x} = lim_{k\\rightarrow\\infty}\\lambda_1^k\\beta_1\\bar{v}_1$\n",
      "\n",
      "The final equation convergest to $0$ if $|\\lambda_1| < 1$ and diverges if $|\\lambda_1| >1$.\n",
      "\n",
      "Looking at the final relationship we can scale the powers of $A^k\\bar{x}$ in a manner which ensures that the limit is always finite and non-zero. The scaling begins by chosing $\\bar{x}$ to be a unit vector, $\\bar{x}^{(0)}$, relative to the infinity norm and chosing a component $x_{p_0}^{(0)}$ of $\\bar{x}^{(0)}$ such that:\n",
      "\n",
      "$x_{p_0}^{(0)}= 1 = ||\\bar{x}^{(0)}||_\\infty$\n",
      "\n",
      "Now, we let $\\bar{y}^{(1)}=A\\bar{x}^{(0)}$, and define $\\mu^{(1)} = y_{p_0}^{(1)}$. Then,\n",
      "\n",
      "$$\n",
      "\\mu^{(1)} = y_{p_0}^{(1)} = \\frac{y_{p_0}^{(1)}}{x_{p_0}^{(0)}} = \\frac{\\beta_1\\lambda_1 v_{p_0}^{(1)} + \\sum_{j=2}^n \\beta_j \\lambda_j v_{p_0}^{(j)}}{\\beta_1 v_{p_0}^{(1)} + \\sum_{j=2}^n \\beta_j v_{p_0}^{(j)}} = \\lambda_1 \\left[\\frac{\\beta_1 \\lambda_1 v_{p_0}^{(1)} + \\sum_{j=2}^n \\beta_j (\\frac{\\lambda_j}{\\lambda_1}) v_{p_0}^{(j)}}{\\beta_1 v_{p_0}^{(1)} + \\sum_{j=2}^n \\beta_j v_{p_0}^{(j)}}\\right]\n",
      "$$\n",
      "\n",
      "Let $p_1$ be the smallest integer such that\n",
      "\n",
      "$|y_{p_1}^{(1)}| = ||\\bar{y}^{(1)}||_\\infty$\n",
      "\n",
      "finally we define $\\bar{x}^{(1)}$ as \n",
      "\n",
      "$\\bar{x}^{(1)} = \\frac{1}{y_{p_1}^{(1)}}\\bar{y}^{(1)} = \\frac{1}{y_{p_1}^{(1)}}A\\bar{x}^{(0)}$\n",
      "\n",
      "then,\n",
      "\n",
      "$x_{p_1}^{(1)} = 1 = ||\\bar{x}^{(1)}||_\\infty$\n",
      "\n",
      "Now define,\n",
      "\n",
      "$\\bar{y}^{(2)} = A \\bar{x}^{(1)} = A^2 \\bar{x}^{(0)}$\n",
      "\n",
      "and\n",
      "\n",
      "$\\mu^{(2)} = y_{p_1}^{(2)} = \\frac{y_{p_1}^{(2)}}{x_{p_1}^{(1)}} = \\frac{[\\beta_1 \\lambda_1^2 v_{p_1}^{(1)}+ \\sum_{j=2}^n \\beta_j \\lambda_j^2 v_{p_1}^{(j)}]/y_{p_1}^{(1)}}{[\\beta_1 \\lambda_1 v_{p_1}^{(1)} + \\sum_{j=2}^n \\beta_j \\lambda_j v_{p_1}^{(j)}]/y_{p_1}^{(1)}} = \\lambda_1 \\left[ \\frac{\\beta_1 v_{p_1}^{(1)} + \\sum_{j=2}^n \\beta_j (\\lambda_j/\\lambda_1)^2 v_{p_1}^{(j)}}{\\beta_1 v_{p_1}^{(1)}+\\sum_{j=2}^n \\beta_j (\\lambda_j/\\lambda_1) v_{p_1}^{(j)}}\\right]$\n",
      "\n",
      "Let $p_2$ be the smallest integer such that\n",
      "\n",
      "$|y_{p_2}^{(2)}| = ||\\bar{y}^{(2)}||_\\infty$\n",
      "\n",
      "finally we define $\\bar{x}^{(1)}$ as\n",
      "\n",
      "$\\bar{x}^{(2)} = \\frac{1}{y_{p_2}^{(2)}}\\bar{y}^{(2)} = \\frac{1}{y_{p_1}^{(2)}}A\\bar{x}^{(1)} = \\frac{1}{y_{p_1}^{(2)}y_{p_1}^{(1)}}A^2\\bar{x}^{(0)}$\n",
      "\n",
      "Continuing this sequence we have, \n",
      "\n",
      "$\\bar{y}^{(m)} = A\\bar{x}^{(m-1)}$\n",
      "\n",
      "$\\mu^{(m)} = y_{p_{m-1}}^{(m)} = \\frac{y_{p_0}^{(2)}}{x_{p_0}^{(1)}} = \\lambda_1 \\left[\\frac{\\beta_1 v_{p_{m-1}}^{(1)} + \\sum_{j=2}^n \\beta_j (\\frac{\\lambda_j}{\\lambda_1})^m v_{p_{m-1}}^{(j)}}{\\beta_1 v_{p_{m-1}}^{(1)} + \\sum_{j=2}^n \\beta_j (\\frac{\\lambda_j}{\\lambda_1}^{m-1} v_{p_{m-1}}^{(j)}}\\right] (*)$\n",
      "\n",
      "and\n",
      "\n",
      "$\\bar{x}^{(m)} = \\frac{\\bar{y}^{(m)}}{y_{p_m}^{(m)}} = \\frac{A^m \\bar{x}^{(0)}}{\\prod_{k=1}^m y_{p_k}^{(k)}}$\n",
      "\n",
      "where at each step, $p_m$ is used to represent the smallest integer for which \n",
      "\n",
      "$|y_{p_m}^{(m)}| = ||\\bar{y}^{(m)}||_\\infty$\n",
      "\n",
      "If we look at $(*)$ we can see that $m \\rightarrow \\infty$, then $\\mu \\rightarrow \\lambda_1$ as long as $\\bar{x}^{(0)}$ is chosen so that $\\beta_1 \\neq 0$. Also, the sequence of vectors $\\bar{x}^{(m)}$ converges to an eigenvector associated with $\\lambda_1$ that has infinity norm of unity. This method has the disadvantage that it will not work if the matrix does not have a single dominant eigenvalue."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Pseudocode for Power Method\n",
      "\n",
      "  1. Initialize an eigenvector, $\\bar{x} = \\{x_1, x_2, x_3\\}$, that is not in the nullspace of $A$.\n",
      "  1. Initialize ERR > TOL. \n",
      "  1. Find the smallest integer $p$ with $1\\leq p\\leq n$ and $|x_p|=||\\bar{x}||_\\infty$, where $n$ is the length of $\\bar{x}$.\n",
      "  1. Set $\\bar{x}=\\frac{\\bar{x}}{x_p}$\n",
      "  1. While ERR>TOL, do Steps 6-10.\n",
      "  1. $\\bar{y} = A\\bar{x}$\n",
      "  1. $\\mu = y_p$\n",
      "  1. Find the smallest integer $p$ with $1\\leq p\\leq n$ and $|y_p| = ||\\bar{y}||_\\infty$\n",
      "  1. Set ERR = $||\\bar{x}-(\\frac{\\bar{y}}{y_p})||$\n",
      "  1. Set $\\bar{x} = \\frac{\\bar{y}}{y_p}$\n",
      "  \n",
      "The rate of convergence for the Power Method is O$(|\\frac{\\lambda_2}{\\lambda_1}|^m)$. Next we will make some modifications to help improve the rate of convergence."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Simulation of Power Method Converging\n",
      "\n",
      "The following simulation shows as Power Law method converging to an eigenvalue and eigenvector with the following $A$ matrix and initial guess for an eigenvector.\n",
      "\n",
      "$$A = \\begin{pmatrix}0.1 & 1\\\\1 & 0.1\\end{pmatrix}\\\\\n",
      "v = \\begin{pmatrix}1\\\\0\\end{pmatrix}$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!!!! SIMULATION PLACE HOLDER !!!!"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Inverse Power Method\n",
      "\n",
      "The *Inverse Power Method* is a modification of the power method that gives faster convergence. It is used to determine the eigenvalue of $A$ that is closest to a specific number $q$.\n",
      "\n",
      "Suppose the matrix $A$ has eigenvalues $\\lambda_1, ..., \\lambda_n$ with linearly indepenedent eigenvectors $\\bar{v}^{(1)},...,\\bar{v}^{(n)}$. The eigenvalues of $(A-qI)^{-1}$, where $q \\neq \\lambda_i$, for $i = 1, 2, ..., n$, are\n",
      "\n",
      "$\\frac{1}{\\lambda_1 - q}, \\frac{1}{\\lambda_2 -q}, ..., \\frac{1}{\\lambda_n - q},$\n",
      "\n",
      "If we apply the power method to $(A- qI)^{-1}$ we have\n",
      "\n",
      "$\\bar{y}^{(m)} = (A-qI)^{-1} \\bar{x}^{(m-1)}$\n",
      "\n",
      "$\\mu^{(m)} = y_{p_{m-1}}^{(m)} = \\frac{y_{p_{p-1}}^{(m)}}{x_{p_{m-1}}^{(m-1)}} = \\frac{\\sum_{j=1}^n \\beta_j \\frac{1}{(\\lambda_j -q)^m}v_{p_{m-1}}^{(j)}}{\\sum_{j=1}^n \\beta_j \\frac{1}{(\\lambda_j-q)^{m-1} v_{p_{m-1}}^{(j)}}}$\n",
      "\n",
      "and\n",
      "\n",
      "$\\bar{x}^{(m)} = \\frac{\\bar{y}^{(m)}}{y_{p_{m}}^{(m)}}$\n",
      "\n",
      "where, at each step, $p_m$ represents the smallest integer for which $|y_{p_{m}}^{(m)}| = ||\\bar{y}^{(m)}||_\\infty$. $\\mu$ with then converge to $\\frac{1}{(\\lambda_k - q)}$, where \n",
      "\n",
      "$\\frac{1}{|\\lambda_k - q|} =$ max$ \\frac{1}{|\\lambda_j - q|}$ where $i = 1, 2, ..., n$\n",
      "\n",
      "and $\\lambda_k \\approx q + \\frac{1}{\\mu^{(m)}}$ is an eigenvalue of $A$ closest to $q$.\n",
      "\n",
      "Once $k$ is known we can write,\n",
      "\n",
      "$\\mu^{(m)} = \\left[\\frac{\\beta_k v_{p_{m-1}}^{(k)} + \\sum_{j=1}^n \\beta_j \\left[\\frac{\\lambda_k -q}{\\lambda_j -q}\\right]^m v_{p_{m-1}}^{(j)}}{\\beta_k v_{p_{m-1}}^{(k)} + \\sum_{j=1}^n \\beta_j\\left[\\frac{\\lambda_k -q}{\\lambda_j -q}\\right]^{(m-1)} v_{p_{m-1}}^{(j)}}\\right]$\n",
      "\n",
      "The choice of $q$ determines the convergence, provided that $\\frac{1}{\\lambda_k -q}$ is a dominant eigenvalue of $(A-qI)^{-1}$. The closer $q$ is to an eigenvalue $\\lambda_k$, the faster the onvergence. The convergence is on the order\n",
      "\n",
      "O$\\left(\\left(\\left|\\frac{(\\lambda-q)^{-1}}{(\\lambda_k -q)^{-1}}\\right|\\right)^m\\right)=$O$\\left(\\left(\\left|\\frac{(\\lambda_k-q)}{(\\lambda-q)}\\right|\\right)^m\\right)$\n",
      "\n",
      "where $\\lambda$ represents the eigenvalue of $A$ that is second closest to $q$.\n",
      "\n",
      "The vector $\\bar{y}^{(m)}$ is obtained by solving the equation,\n",
      "\n",
      "$(A-qI)\\bar{y}^{(m)} = \\bar{x}^{(m-1)}$\n",
      "\n",
      "This equation can be solved by any of the methods we previously learned for solving linear systems. The selection of $q$ can be based on some means of localizing an eigenvalue (e.g. Gerschogrin Circle, etc.) If we have an initial approximation of the eigenvector, $\\bar{x}^{(0)}$, we can choose $q$, as such\n",
      "\n",
      "$q = \\frac{\\bar{x}^{(0)T}A\\bar{x}^{(0)}}{\\bar{x}^{(0)T} \\bar{x}^{(0)}}$\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Pseudocode for Inverse Power Method.\n",
      "\n",
      "  1. Initialize an eigenvector, $\\bar{x} = \\{x_1, x_2, x_3\\}$, that is not in the null space of $A$.\n",
      "  1. Initialize ERR > TOL. \n",
      "  1. Set $q = \\frac{\\bar{x}^T A \\bar{x}}{\\bar{x}^T \\bar{x}}$\n",
      "  1. Find the smallest integer $p$ with $1\\leq p \\leq n$ and $|x_p| = ||\\bar{x}||_\\infty$, where $n$ is the length of $\\bar{x}$.\n",
      "  1. Set $\\bar{x} = \\frac{\\bar{x}}{x_p}$\n",
      "  1. While ERR > TOL, do steps 7-11\n",
      "  1. Solve $(A-qI)\\bar{y} = \\bar{x}$\n",
      "  1. $\\mu = y_p$\n",
      "  1. Find the smallest integer $p$ with $1 \\leq p \\leq n$ and $|y_p| = ||\\bar{y}||_\\infty$\n",
      "  1. Set ERR = $||\\bar{x}-(\\frac{\\bar{y}}{y_p})||$\n",
      "  1. Set $\\bar{x} = \\frac{\\bar{y}}{y_p}$\n",
      "  1. Set $\\mu = (\\frac{1}{\\mu}) + q$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Deflation techniques\n",
      "\n",
      "There are several ways of determining approximation to the other eigenvalues once a dominant eigenvalue is known. A class of techniques called *deflation techniques* involve forming a new matrix $B$ with eigenvalues idential to those in $A$ with the dominant eigenvalue of $A$ replaced with $0$. There are then a series of steps that allow you to find the remaining eigenvalues. One poplular deflation technique is called *Wieland deflation*. These deflation techniques tend to be very susceptible of roundoff error and usually they are only used in approximating the initial guess, q, for use with the Inverse Power Method. When all the eigenvalues of a matrix are required there are better methods for achieving their approximations. Therefore, we will fore go a discussion of deflation techniques and move on to these other methods which involve similarity transformations."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}